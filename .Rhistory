# The p-values of the tests are calculated using the Chi-square distribution.
#----------------------------------------------------------------------------------------------
# More simply,a likelihood ratio test comparing the full and reduced models can be performed
# using the anova() function with the additional option test="Chisq".
anova(logit.noBK, logit.overall, test = "Chisq")
#The p-value for the drop in deviance test is < 0.001, which is quite significant.
#We reject the null hypothesis that the coefficient for the birdkeeping variable
#is 0, and conclude that having it in the model should provide a better fit.
#i.e., the full model is better than the reduced model
pchisq(logit.justBKYR$deviance - logit.overall$deviance,
logit.justBKYR$df.residual - logit.overall$df.residual,
lower.tail = FALSE)
logit.overall = glm(LC ~ ., family = "binomial", data = case2002)
# Note that the option family is set to binomial,
# which tells R to perform logistic regression.
# H0: The logistic regression model is appropriate.
# H1: The logistic regression model is not appropriate.
pchisq(logit.overall$deviance, logit.overall$df.residual, lower.tail = FALSE)
# Why do we use lower.tail here?
# http://stats.stackexchange.com/questions/22347/is-chi-squared-always-a-one-sided-test
#The p-value for the overall goodness of fit test is about 0.20,
#which is greater than the cutoff value of 0.05. We do not have
#evidence to reject the null hypothesis that the model is appropriate.
logit.overall$coefficients
# ---- OR -----
summary(logit.overall)
#The log odds of having lung cancer are increased by about 0.56 if you
#are a female as compared to a male, holding all other variables constant.
logit.noBK = glm(LC ~ . - BK, family = "binomial", data = case2002)
#overall goodness of fit test
pchisq(logit.noBK$deviance, logit.noBK$df.residual, lower.tail = FALSE)
#The p-value for the overall goodness of fit test is about 0.07, which is greater
#than the cutoff value of 0.05. We do not have evidence to reject the null
#hypothesis that the model is appropriate.
pchisq(logit.noBK$deviance - logit.overall$deviance,
logit.noBK$df.residual - logit.overall$df.residual,
lower.tail = FALSE)
#------------------------------------------------Note-----------------------------------------
# Likelihood ratio tests are similar to partial F-tests in the sense that they compare
# the full model with a restricted model where the explanatory variables of interest are omitted.
# The p-values of the tests are calculated using the Chi-square distribution.
#----------------------------------------------------------------------------------------------
# More simply,a likelihood ratio test comparing the full and reduced models can be performed
# using the anova() function with the additional option test="Chisq".
anova(logit.noBK, logit.overall, test = "Chisq")
#The p-value for the drop in deviance test is < 0.001, which is quite significant.
#We reject the null hypothesis that the coefficient for the birdkeeping variable
#is 0, and conclude that having it in the model should provide a better fit.
#i.e., the full model is better than the reduced model
logit.justBKYR = glm(LC ~ BK + YR, family = "binomial", data = case2002)
pchisq(logit.justBKYR$deviance - logit.overall$deviance,
logit.justBKYR$df.residual - logit.overall$df.residual,
lower.tail = FALSE)
#---------OR---------------
anova(logit.justBKYR, logit.overall, test = "Chisq")
#The p-value for the drop in deviance test is quite large at 0.4175, indicating
#that the null hypothesis that the coefficients of gender, socioeconomic status,
#age, and average rate of smoking are jointly not adding any predictive power
#to our analysis. The reduced model is sufficient.
AIC(logit.overall, logit.noBK, logit.justBKYR)
BIC(logit.overall, logit.noBK, logit.justBKYR)
1 - logit.overall$deviance/logit.overall$null.deviance
1 - logit.noBK$deviance/logit.noBK$null.deviance
1 - logit.justBKYR$deviance/logit.justBKYR$null.deviance
#The AIC and BIC are minimized for the most simplified model, indicating that it
#is most preferable. While the McFadden's R^2 term isn't maximized for the most
#simplified model (only about 15.5% as compared to about 17.6% for the overall
#model with all coefficients), we choose to move forward with the simplified
#model because it has relatively high predictive power alongside simplicity.
#---------------------How to interpret the McFadden's R^2------------------------
#http://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation
newdata = with(case2002, data.frame(YR = mean(YR),
BK = factor(c("Bird", "NoBird"))))
cbind(newdata, "Prob. Lung Cancer" = predict(logit.justBKYR, newdata, type = "response"))
newdata = data.frame(YR = 0, BK = factor(c("Bird", "NoBird")))
cbind(newdata, "Prob. Lung Cancer" = predict(logit.justBKYR, newdata, type = "response"))
#------------------------------------------Note---------------------------------------
# The usage is similar to that of the function predict which we previously used when working
# on multiple linear regression problems. The main difference is the option type, which tells
# R which type of prediction is required.
#-------------------------------------------------------------------------------------------
# The default predictions are given on the logit scale (i.e.predictions are made in terms of the log odds),
# while using type = "response"
LC.predicted = round(logit.justBKYR$fitted.values)
table(truth = case2002$LC, prediction = LC.predicted)
(85+22)/147
98/147
#The model performs decently well with an approximate 72.79% accuracy; this is
#a bit above the baseline of 66.67% accuracy if we were to simply guess that
#every individual has no lung cancer within our dataset.
# Why do we choose the larger group 98/147 as our baseline?
# Because if we don't do any analysis on this and a new sample comes in,
# we will assign the most likely category to the new one. So this is the base line.
X = case2002$YR
is_LC = as.numeric(case2002$LC == 'LungCancer')
neg_log_likelihood<-function(intercept, slope){
t = slope*X+intercept
prob = 1/(1+exp(-t))
e = 1e-8
tmp = sapply(is_LC*prob + (1-is_LC)*(1-prob), function(i) min(max(i, e), 1-e))
return(-1*sum(log(tmp)))
}
#If you implement correctly, the code below:
# install.packages('optimx')
library(optimx)
install.packages("glmnet")
install.packages("corrplot")
restaurants = read.table("https://s3.amazonaws.com/nycdsabt01/NYC_Restaurants.txt")
plot(restaurants[, -c(1, 6)], col = restaurants$Location)
model.saturated = lm(Price ~ Food + Decor + Service + Location, data = restaurants)
summary(model.saturated)
#The regression equation is: Price = -21.96 + 1.54*Food + 1.91*Decor - 0.003*Service
#- 2.07*West.
#
#When all ratings are 0 and the restaurant is on the East side, the
#average expected price of dinner is about $-21.96 (this does not make sense in
#context of this problem).
#
#As the food rating for a restaurant increases by one
#point, the price increases on average by about $1.54 WHILE HOLDING ALL OTHER
#VARIABLES CONSTANT.
#
#As the decor rating for a restaurant increases by one
#point, the price increases on average by about $1.91 WHILE HOLDING ALL OTHER
#VARIABLES CONSTANT.
#
#As the service rating for a restaurant increases by one
#point, the price decreases on average by about $0.003 WHILE HOLDING ALL OTHER
#VARIABLES CONSTANT.
#
#A restaurant on the west side has an average dinner price that is about $2.07
#cheaper than a meal on the east side WHILE HOLDING ALL OTHER VARIABLES CONSTANT.
#
#All coefficients are significant except for the Service coefficient.
#
#The overall F-statistic is significant so the overall regression is significant.
#
#The RSE is about 5.738, which is an estimate of the average deviation of the
#observations around the regression line.
#
#The adjusted R^2 is 0.6187, meaning that approximately 61.87% of the variation
#in dinner price is accounted for by the variables in our model.
plot(model.saturated)
#No overt deviations from any of the assumptions.
library(car)
influencePlot(model.saturated)
#There are a few points that surface with either high residuals or high hat-values,
#but there are none that have a severe negative influence on the regression surface.
vif(model.saturated)
#The VIF of Service is about 3.6, which is decently high. As we saw in the
#scatterplot matrix above, we might be cautious about multicollinearity in our
#data because of correlations among our predictor variables.
avPlots(model.saturated)
#The food, decor, and location variables all seem to add some type of information
#to our model; however, the service variable doesn't seem to add any information
#when among the other predictors in this model.
model.service = lm(Price ~ Service, data = restaurants)
summary(model.service)
influencePlot(model.service)
#In the simple linear regression model, service is a highly significant variable
#that helps to predict the price of dinner; however, because of multicollinearity
#with other variables such as the food rating, in a multiple regression setting
#this variable tends to fall out of significance because it doesn't add any
#additional information.
### Model 1: Food, Decor ###
model1 = lm(Price ~ Food + Decor, data = restaurants)
summary(model1)
plot(model1)
influencePlot(model1)
vif(model1)
avPlots(model1)
#All coefficients are significant, and the overall regression is significant. The
#adjusted R^2 value increased a bit while the RSE decreased slightly. There are
#a couple observations that are outliers but there are no extreme deviations from
#any of the regression assumptions. The VIF and added variable plots show that
#we don't have any cause for concern of multicollinearity in our model.
### Model 2: Food, Decor, Location ###
model2 = lm(Price ~ Food + Decor + Location, data = restaurants)
summary(model2)
plot(model2)
influencePlot(model2)
vif(model2)
avPlots(model2)
#Similar to before...all coefficients are significant, although location is less
#significant than the other coefficients. The overall regression is significant.
#adjusted R^2 value increased a bit while the RSE decreased slightly. There are
#a couple observations that are outliers but there are no extreme deviations from
#any of the regression assumptions. The VIF and added variable plots show that
#we don't have any cause for concern of multicollinearity in our model.
### FINAL Model comparison ###
#AIC
AIC(model.saturated, model1, model2, model.service)
#BIC
BIC(model.saturated, model1, model2, model.service)
#model 2 has the least AIC, model 1 has the least BIC. All are relatively close,
#so you could make an argument for any one of them.
#MLR Assumptions: Linearity, constant variance, normality, independent errors, no multicollinearity.
#Multicollinearity can inflate the standard errors of your estimated coefficients.
#Also, the power and reliability of your coefficients will decrease.
n =1000
tot_iter = 1000
count_=0
for(i in 1:tot_iter){
x = runif(n=n)
error = rnorm(n = n)
y =  error
data = data.frame(x = x, y = y)
mod = lm(y ~ x, data=data)
count_ = count_ + (summary(mod)$coefficients['x', 4]<=0.05)
}
print(count_/tot_iter)
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
x = model.matrix(lpsa ~ ., prostate)[, -1]
y = prostate$lpsa
set.seed(0)
train = sample(1:nrow(x), nrow(x)*0.8)
test = (-train)
y.test = y[test]
length(train)/nrow(x)  # 0.7938144
length(y.test)/nrow(x) # 0.2061856
grid = 10^seq(5, -2, length = 100)
#  Fit the ridge regression. Alpha = 0 for ridge regression.
library(glmnet)
ridge.models = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
#  Comment on the shrinkage
#The coefficients all seem to shrink towards 0 as lambda gets quite large.
#Most coefficients are very close to 0 once the log lambda value gets to about 5.
#However, in ridge regression coefficients are never exactly 0.
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train], alpha = 0, nfolds = 10, lambda = grid)
plot(cv.ridge.out, main = "Ridge Regression\n")
#The error seems to be reduced with a log lambda value of around -2.0002;
#this corresponts to a lambda value of about 0.135. This is the value of lambda
#we should move forward with in our future analyses.
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge       # 0.1353048
log(bestlambda.ridge)  # -2.000225
ridge.bestlambdatrain = predict(ridge.models, s = bestlambda.ridge, newx = x[test, ])
mean((ridge.bestlambdatrain - y.test)^2)  # 0.4913108
#The test MSE is about 0.4913 with the best lambda.
ridge.best_refit = glmnet(x, y, alpha = 0, lambda = bestlambda.ridge)
#  Coefficients
predict(ridge.best_refit, s = bestlambda.ridge, type = "coefficients")
#(Intercept) -0.038129777
#lcavol       0.460638683
#lweight      0.591222480
#age         -0.014700329
#lbph         0.081022541
#svi          0.653878622
#lcp         -0.014662260
#gleason      0.068111379
#pgg45        0.003106411
#The coefficients have been shrunken towards 0. Most notably, it appears as though
#the pgg45 veriable has been reduced the most, with a coefficient of only about 0.0031.
#On the other hand, the svi, lweight, and lcavol have the highest coefficient values (all greater than 0.46), indicating that they are the stronger predictors.
#NB: We cannot interpret these values directly because of the nature of the
#regularization process.
# MSE
ridge.bestlambda = predict(ridge.best_refit, s = bestlambda.ridge, newx = x)
mean((ridge.bestlambda - y)^2)  # 0.4549825
#The overall MSE is about 0.4550, which is a bit smaller than the test MSE 0.4913.
#In general, the overall error rate calculated on the whole data is often underestimated simply because it incorporates the data in the construction of the model in the first place.
#Lambda is the penalty coefficient in Ridge, Lasso, and Elastic Net regression.
#The value of lambda determines the relative impact of the RSS and shinkage penalty term on the coefficient estimates to prevent the model from overfitting.
#We use L2 and L1 norm regularization for the Ridge and Lasso regression respectively.
#The Elastic Net penalty is a combination of L1 and L2 penalization.
#The main purpose of cross validation is to get a sense of how the model might perform  on data that it has not seen yet. Without cross validation we only have information onhow our model performs on our in-sample data. By doing cross validation, we can check how the model performs when we have new data.
#Kfold Process
#1. Divide the training set into k folds
#2. For each fold j from 1 to k, hold fold j out and train one model on the data       from the remaining k-1 folds. Then evaluate the model on the fold j that was       held out. Typical evaluation metrics include MSE for regression and logloss or     accuracy for classification.
#3. Take the average of the errors from part 2 over the k folds. This is your cross-validation error.
#Empirically, using k=5 or k=10 yields test error rate estimates that suffer neither from excessively high bias nor from very high variance. There is a bias-variance tradeoff associated with the choice of k in k-fold cross-validation.
#Larger k-fold means less bias towards overestimating the true expected error (as training folds will be closer to the total dataset) but higher variance and higher running time (as you are getting closer to Leave-One-Out CV, the limit case).
#https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation?answertab=oldest#tab-top
##3.1 Repeat the entire analysis using the lasso regression method.
#3 Fit a model
#Fitting the lasso regression. Alpha = 1 for lasso regression.
lasso.models = glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
#4 Visualization
#  Plot the coefficients of these models
plot(lasso.models, xvar = "lambda", label = TRUE, main = "Lasso Regression")
#The coefficients all seem to shrink towards 0 as lambda gets quite large.
#All coefficients seem to go to 0 once the log lambda value gets to about 0.
#Note that coefficients are necessarily set to exactly 0 for lasso regression.
#5 Cross Validation
#  Perform 10-fold cross validation and use set.seed(0) on the training data
set.seed(0)
cv.lasso.out = cv.glmnet(x[train, ], y[train], alpha = 1, nfolds = 10, lambda = grid)
#6 Visualization
#  Create a plot associated with the 10-fold cross validation
plot(cv.lasso.out, main = "Lasso Regression\n")
#7 Results
bestlambda.lasso = cv.lasso.out$lambda.min
bestlambda.lasso       # 0.0367838
log(bestlambda.lasso)  # -3.302698
#The error seems to be reduced with a log lambda value of around -3.3027.
#This corresponts to a lambda value of about 0.0368, which is the value of lambda
#we should move forward with in our future analyses.
#8 Fit a model
#  Fit a lasso regression model using the best lambda on the test dataset.
lasso.bestlambdatrain = predict(lasso.models, s = bestlambda.lasso, newx = x[test, ])
mean((lasso.bestlambdatrain - y.test)^2)  # 0.5060285
#The test MSE is about 0.5060 with the best lambda.
#9 Refit a model & Results
lasso.best_refit = glmnet(x, y, alpha = 1)
# Coefficients
predict(lasso.best_refit, type = "coefficients", s = bestlambda.lasso)
#(Intercept)  0.1442980764
#lcavol       0.5066665625
#lweight      0.5433976279
#age         -0.0080716635
#lbph         0.0607690369
#svi          0.5885646964
#lcp          .
#gleason      0.0006617546
#pgg45        0.0022821317
#The coefficients have been shrunken towards 0; most notably, the lcp variable
#has dropped out first and has a coefficient of exactly 0. Other variables like
#gleason, pgg45, and age have pretty small coefficient values as well.
#Similar to the ridge regression scenario, the svi, lweight, and lcavol all have
#the largest coefficient estimates.
# MSE
lasso.bestlambda = predict(lasso.best_refit, s = bestlambda.lasso, newx = x)
mean((lasso.bestlambda - y)^2)  # 0.4599577
#The overall MSE is about 0.45996. Again, this is similar to the test MSE 0.506
#we found above, but a little bit lower due to the way in which the model was
#fit using the data at hand.
predict(ridge.best_refit, type = "coefficients", s = bestlambda.ridge)
predict(lasso.best_refit, type = "coefficients", s = bestlambda.lasso)
mean((ridge.bestlambda - y)^2)  # 0.4549825
mean((lasso.bestlambda - y)^2)  # 0.4599577
#Both models appear to perform in a similar manner. Both the test MSEs and the
#overall MSEs were approximately the same. Ultimately, the ridge regression MSE
#was slightly lower than that of the lasso. Although this might be due to random
#variability among our dataset, if we are strictly talking about "predictive"
#power, we might choose to move forth with the ridge regression in this scenario.
#On the other hand, the final lasso regression model drop the lcp variable altogether (setting its coefficient value to 0). If we are particularly interested in "dimension reduction and variable selection", we might favor of the lasso regression.
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
#  Comment on the shrinkage
#The coefficients all seem to shrink towards 0 as lambda gets quite large.
#Most coefficients are very close to 0 once the log lambda value gets to about 5.
#However, in ridge regression coefficients are never exactly 0.
update.packages(ask = FALSE, checkBuilt = TRUE)
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("rlang")
install.packages(c("boot", "class", "dplyr", "ellipsis", "foreign", "glue", "KernSmooth", "MASS", "nlme", "nnet", "pkgload", "spatial", "V8", "vctrs", "VIM"))
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("shiny", dependencies = FALSE)
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("R6", dependencies = FALSE)
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("digest", dependencies = FALSE)
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("htmltools", dependencies = FALSE)
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("rcpp", dependencies = FALSE)
install.packages("Rcpp", dependencies = FALSE)
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("httpuv", dependencies = FALSE)
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("later", dependencies = FALSE)
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("shiny")
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("promises")
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("magrittr")
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("mime")
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("xtable")
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("fastmap")
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("shinydashboard")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("shinyjs")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("shinythemes")
install.packages(c('dashboardthemes', 'shinyWidgets', 'shinydashboardPlus', 'scales', 'dplyr', 'viridis', 'lubridate', 'ggplot2',
'ggthemes', 'DT', 'data.table', 'jsonlite', 'ggpubr', 'highcharter', 'bs4Dash'))
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("lifecycle")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("munsell")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("colorspace")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("assertthat")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("dplyr")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("pkgconfig")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("httpuv")
install.packages("Rcpp", dependencies = TRUE, INSTALL_opts = '--no-lock')
install.packages("Rcpp", dependencies = TRUE, INSTALL_opts = "--no-lock")
install.packages("Rcpp", dependencies = TRUE, INSTALL_opts = "--no-lock")
install.packages("httpuv", dependencies = TRUE, INSTALL_opts = '--no-lock')
install.packages("Rcpp")
install.packages("httpuv")
install.packages("Rcpp")
install.packages("Rcpp")
install.packages("Rcpp")
install.packages("httpuv", dependencies = TRUE, INSTALL_opts = '--no-lock')
vvv
library(Rcpp)
remove.packages("Rcpp")
library(httpuv)
remove.packages("httpuv")
install.packages("Rcpp", dependencies = TRUE, INSTALL_opts = "--no-lock")
install.packages("Rcpp", dependencies = TRUE, INSTALL_opts = "--no-lock")
install.packages("httpuv", dependencies = TRUE, INSTALL_opts = '--no-lock')
library(httpuv)
remove.packages("httpuv")
install.packages("httpuv")
install.packages("httpuv")
devtools::install_github("rstudio/httpuv")
install.packages("httpuv")
install.packages("httpuv")
install.packages("httpuv")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
install.packages("jsonlite", type = "source")
install.packages("shiny")
devtools::load_all(".")
install.packages("github")
install.packages("devtools")
library(shiny)
remove.packages("shiny")
remove.packages(shiny)
.libPaths()
install.packages("shiny")
install.packages("BH", repos="http://R-Forge.R-project.org")
install.packages("shiny")
install.packages("httpuv")
devtools::install_version("httpuv", "1.3.5")
install.packages('httpuv', type='binary')
install.packages('shiny', type='binary')
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
install.packages("readr")
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
traceback()
traceback()
runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
shiny::runApp('E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis')
setwd("E:/SCHOOL/00-nydsa/W1/Shiny-App-DataBreach/shinyapp-databreach-analysis")
runApp()
runApp()
runApp()
p1 <- p + theme_wsj() + scale_colour_wsj("colors6")
py1 <- plotly()
out1 <- py1$ggplotly(p1, kwargs = list(filename = "PlotlyThemeExample", fileopt = "overwrite"))
url1 <- out1$response$url
p1<- pbreach %>%
group_by(Year, Month) %>%
count(region,sort = TRUE)
p1 <- p + theme_wsj() + scale_colour_wsj("colors6")
py1 <- plotly()
out1 <- py1$ggplotly(p1, kwargs = list(filename = "PlotlyThemeExample", fileopt = "overwrite"))
url1 <- out1$response$url
# data <- fromJSON(file="https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json")
library(gapminder)
p <- gapminder %>%
filter(year==1977) %>%
ggplot( aes(gdpPercap, lifeExp, size = pop, color=continent)) +
geom_point() +
theme_bw()
# data <- fromJSON(file="https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json")
library(gapminder)
p <- gapminder %>%
filter(year==1977) %>%
ggplot( aes(gdpPercap, lifeExp, size = pop, color=continent)) +
geom_point() +
theme_bw()
p1<- pbreach %>%
group_by(Year, Month) %>%
count(region,sort = TRUE) %>%
ggplot( aes(Year, region, color=continent)) +
geom_bar() +
theme_bw()
p1
p1<- pbreach %>%
group_by(Year, Month) %>%
count(region,sort = TRUE) %>%
ggplot( aes(Year, region, color=region)) +
geom_bar() +
theme_bw()
p1
p1<- pbreach %>%
group_by(Year, Month) %>%
ggplot( aes(Year, region, color=region)) +
geom_bar() +
theme_bw()
p1
pbreach %>%
ggplot( aes(Year, region, color=region)) +
geom_bar() +
theme_bw()
pbreach %>%
ggplot( aes(Year, region, color=region)) +
geom_bar() +
theme_bw()
runApp()
runApp()
runApp()
View(boptions)
runApp()
